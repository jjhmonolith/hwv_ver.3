# Phase 4b: Voice Interview

## Overview
- **ëª©í‘œ**: ìŒì„± ëª¨ë“œ ì¸í„°ë·° ê¸°ëŠ¥ ì¶”ê°€ (TTS + STT)
- **ì˜ˆìƒ íŒŒì¼ ìˆ˜**: 6ê°œ (Backend 2 + Frontend 4)
- **ì˜ì¡´ì„±**: Phase 4a ì™„ë£Œ

---

## Checklist

### Backend (TypeScript)
- [ ] `backend/src/routes/speech.ts` - TTS/STT API
- [ ] `backend/src/services/speech.ts` - ElevenLabs TTS + Whisper STT

### Frontend (TypeScript)
- [ ] `frontend/hooks/useSpeech.ts` - TTS/STT í›… (í•µì‹¬ í›…)
- [ ] `frontend/components/interview/VoiceInterface.tsx` - ìŒì„± ëª¨ë“œ UI
- [ ] `frontend/components/interview/VolumeVisualizer.tsx` - ë³¼ë¥¨ ì‹œê°í™”
- [ ] `frontend/components/interview/RecordButton.tsx` - ë…¹ìŒ ë²„íŠ¼

---

## Files to Create/Modify

| íŒŒì¼ | ì„¤ëª… | ìƒíƒœ |
|------|------|------|
| `backend/src/routes/speech.ts` | /tts, /stt, /status | â¬œ |
| `backend/src/services/speech.ts` | ElevenLabs, Whisper í†µí•© | â¬œ |
| `frontend/hooks/useSpeech.ts` | speak(), startListening(), stopListening() | â¬œ |
| `frontend/components/interview/VoiceInterface.tsx` | ë…¹ìŒ ë²„íŠ¼, ìƒíƒœ í‘œì‹œ | â¬œ |
| `frontend/components/interview/VolumeVisualizer.tsx` | ì‹¤ì‹œê°„ ë³¼ë¥¨ ë§‰ëŒ€ | â¬œ |
| `frontend/components/interview/RecordButton.tsx` | ëˆ„ë¥´ê³  ë§í•˜ê¸° ë²„íŠ¼ | â¬œ |

---

## API Endpoints (ì°¸ì¡°: 05-api.md)

### Speech API
| Method | Endpoint | ì„¤ëª… |
|--------|----------|------|
| GET | `/api/speech/status` | ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ |
| POST | `/api/speech/tts` | í…ìŠ¤íŠ¸ â†’ ìŒì„± |
| POST | `/api/speech/stt` | ìŒì„± â†’ í…ìŠ¤íŠ¸ |

---

## Key Implementation Details

### TTS ì„œë¹„ìŠ¤ (ElevenLabs)
```typescript
// services/speech.ts
import { ElevenLabsClient } from 'elevenlabs';

const client = new ElevenLabsClient({
  apiKey: process.env.ELEVENLABS_API_KEY
});

export async function textToSpeech(text: string): Promise<Buffer> {
  const audio = await client.generate({
    voice: process.env.ELEVENLABS_VOICE_ID,
    model_id: process.env.ELEVENLABS_MODEL,
    text
  });

  const chunks: Buffer[] = [];
  for await (const chunk of audio) {
    chunks.push(chunk);
  }

  return Buffer.concat(chunks);
}
```

### STT ì„œë¹„ìŠ¤ (Whisper)
```typescript
// services/speech.ts
import OpenAI from 'openai';

const openai = new OpenAI();

export async function speechToText(
  audioBuffer: Buffer,
  context?: string
): Promise<string> {
  const file = new File([audioBuffer], 'audio.webm', { type: 'audio/webm' });

  const response = await openai.audio.transcriptions.create({
    file,
    model: 'whisper-1',
    prompt: context, // Context-aware íŒíŠ¸
    language: 'ko'
  });

  return response.text;
}
```

### useSpeech í›… (í•µì‹¬ í›…)
```typescript
// hooks/useSpeech.ts

// TTS
export function useSpeechSynthesis() {
  const [isSpeaking, setIsSpeaking] = useState(false);
  const abortControllerRef = useRef<AbortController | null>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);

  const speak = async (text: string) => {
    // ì´ì „ ìš”ì²­ ì·¨ì†Œ
    abortControllerRef.current?.abort();
    abortControllerRef.current = new AbortController();

    setIsSpeaking(true);

    const response = await fetch('/api/speech/tts', {
      method: 'POST',
      body: JSON.stringify({ text }),
      signal: abortControllerRef.current.signal
    });

    const blob = await response.blob();
    const audio = new Audio(URL.createObjectURL(blob));
    audioRef.current = audio;

    audio.onended = () => {
      setIsSpeaking(false);
      URL.revokeObjectURL(audio.src);
    };

    await audio.play();
  };

  return { isSpeaking, speak, stop };
}

// STT
export function useWhisperRecognition() {
  const [isListening, setIsListening] = useState(false);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [volumeLevel, setVolumeLevel] = useState(0);

  const startListening = async (context?: string) => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

    // AudioContextë¡œ ë³¼ë¥¨ ì‹œê°í™”
    const audioContext = new AudioContext();
    const analyser = audioContext.createAnalyser();
    // ... ë³¼ë¥¨ ëª¨ë‹ˆí„°ë§

    // MediaRecorderë¡œ ë…¹ìŒ
    const recorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
    recorder.start();
    setIsListening(true);
  };

  const stopListening = async (): Promise<string> => {
    setIsListening(false);
    setIsTranscribing(true);

    // ë…¹ìŒ ë°ì´í„°ë¥¼ ì„œë²„ë¡œ ì „ì†¡
    const formData = new FormData();
    formData.append('audio', audioBlob);
    formData.append('context', context);

    const response = await fetch('/api/speech/stt', {
      method: 'POST',
      body: formData
    });

    const { text } = await response.json();
    setIsTranscribing(false);
    return text;
  };

  return { isListening, isTranscribing, volumeLevel, startListening, stopListening };
}
```

### Turn State Guard
```typescript
// ì¤‘ë³µ ì œì¶œ ë°©ì§€
const [turnSubmitted, setTurnSubmitted] = useState(false);

const handleVoiceSubmit = async () => {
  if (turnSubmitted || isTranscribing) return;
  setTurnSubmitted(true);

  try {
    const text = await stopListening();
    if (!text.trim()) return;

    // ë‹µë³€ ì œì¶œ
    const response = await submitAnswer(text);

    // TTSë¡œ ë‹¤ìŒ ì§ˆë¬¸ ì½ê¸°
    await speak(response.next_question);

  } finally {
    setTurnSubmitted(false);
  }
};
```

### Context-aware STT
```typescript
// Whisperì— ì»¨í…ìŠ¤íŠ¸ íŒíŠ¸ ì „ë‹¬
function buildContextForSTT(assignmentText: string, recentQA: Message[]) {
  const excerpt = assignmentText.slice(0, 200);
  const qa = recentQA.slice(-2).map(m => m.content).join(' ');
  return `${excerpt} ${qa}`.trim();
}
```

---

## UI References (ì°¸ì¡°: 01-pages.md)

### ìŒì„± ëª¨ë“œ ë ˆì´ì•„ì›ƒ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì£¼ì œ 1/3: ì„œë¡  ë° ì—°êµ¬ ë°°ê²½              â± 02:45 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ğŸ¤– AI                               [ğŸ”Š]   â”‚   â”‚
â”‚  â”‚ ì§ˆë¬¸ ë‚´ìš©...                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚                    â”‚          â”‚                    â”‚
â”‚                    â”‚   ğŸ¤     â”‚  â† ëˆ„ë¥´ê³  ë§í•˜ê¸°   â”‚
â”‚                    â”‚          â”‚                    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                    â”‚
â”‚           ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ë§í•˜ì„¸ìš”                    â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ìŒì„± ëª¨ë“œ ìƒíƒœ
| ìƒíƒœ | ë²„íŠ¼ | ë©”ì‹œì§€ |
|------|------|--------|
| ëŒ€ê¸° | ğŸ¤ (íŒŒë€ìƒ‰) | ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ë§í•˜ì„¸ìš” |
| ë…¹ìŒ ì¤‘ | ğŸ”´ (ë¹¨ê°„ìƒ‰) | ë…¹ìŒ ì¤‘... (ë³¼ë¥¨ í‘œì‹œ) |
| STT ì²˜ë¦¬ ì¤‘ | â³ | ìŒì„±ì„ ë³€í™˜í•˜ê³  ìˆìŠµë‹ˆë‹¤... |
| TTS ì¬ìƒ ì¤‘ | ğŸ”Š | AIê°€ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤... |

---

## Notes
- ElevenLabs ë¹„ìš© ê´€ë¦¬: ìºì‹± ë˜ëŠ” ì˜ˆì‚° ì œí•œ
- ë¸Œë¼ìš°ì € í˜¸í™˜ì„±: MediaRecorder ì§€ì› í™•ì¸
- ë§ˆì´í¬ ê¶Œí•œ ê±°ë¶€ ì‹œ ì—ëŸ¬ ì²˜ë¦¬
- TTS ì¬ìƒ ì¤‘ íƒ€ì´ë¨¸ ì •ì§€ (Activity-based)
