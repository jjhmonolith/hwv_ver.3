# Phase 4b: Voice Interview

## Overview
- **λ©ν‘**: μμ„± λ¨λ“ μΈν„°λ·° κΈ°λ¥ μ¶”κ°€ (TTS + STT)
- **μμƒ νμΌ μ**: 5κ° (Backend 2 + Frontend 3) - RecordButton μ κ±°λ¨
- **μμ΅΄μ„±**: Phase 4a μ™„λ£
- **μƒνƒ**: β… κµ¬ν„ μ™„λ£ (2026-01-27)

---

## Checklist

### Backend (TypeScript)
- [x] `backend/src/routes/speech.ts` - TTS/STT API
- [x] `backend/src/services/speech.ts` - ElevenLabs TTS + Whisper STT

### Frontend (TypeScript)
- [x] `frontend/hooks/useSpeech.ts` - TTS/STT ν›… (ν•µμ‹¬ ν›…)
- [x] `frontend/components/interview/VoiceInterface.tsx` - μμ„± λ¨λ“ UI (λ‹µλ³€ μ™„λ£ λ²„νΌ ν¬ν•¨)
- [x] `frontend/components/interview/VolumeVisualizer.tsx` - λ³Όλ¥¨ μ‹κ°ν™”

### κΈ°μ΅΄ νμΌ μμ •
- [x] `backend/src/index.ts` - speech λΌμ°νΈ λ“±λ΅
- [x] `backend/.env` - ELEVENLABS ν™κ²½λ³€μ μ¶”κ°€
- [x] `frontend/app/interview/start/page.tsx` - λ§μ΄ν¬ κ¶ν• μ‚¬μ „ μ”μ²­
- [x] `frontend/app/interview/page.tsx` - μμ„± λ¨λ“ ν†µν•©
- [x] `frontend/hooks/useInterviewTimer.ts` - TTS μ¤‘ νƒ€μ΄λ¨Έ μ •μ§€

---

## ν•µμ‹¬ μ •μ±… (λ³€κ²½λ¨)

### μμ„± λ¨λ“ λ™μ‘ μ›μΉ™

| ν•­λ© | μ„¤λ… |
|------|------|
| **λ§μ΄ν¬ μ‹μ‘** | AI μ§λ¬Έ λλ‚λ©΄ **μλ™ μ‹μ‘** (λ²„νΌ λ¶ν•„μ”) |
| **λ‹µλ³€ μΆ…λ£** | [λ‹µλ³€ μ™„λ£] λ²„νΌ ν΄λ¦­ |
| **νƒ€μ΄λ¨Έ (λ…Ήμ μ¤‘)** | **μ‘λ™** (ν•™μƒ λ‹µλ³€ μ‹κ°„) |
| **λ§μ΄ν¬ κ¶ν•** | **μ¤€λΉ„ ν™”λ©΄(/interview/start)μ—μ„ μ‚¬μ „ μ”μ²­** |
| **κ¶ν• μ—†μΌλ©΄** | μΈν„°λ·° μ‹μ‘ λ¶κ°€ (μ±„ν…λ§ κ°€λ¥) |
| **μ¬μ ‘μ† μ‹** | μλ™ λ§μ΄ν¬ μ‹μ‘ |
| **TTS μ‹¤ν¨ μ‹** | μ§λ¬Έ ν…μ¤νΈ ν‘μ‹ + μλ™ λ§μ΄ν¬ μ‹μ‘ |

### νƒ€μ΄λ¨Έ μƒνƒν‘

| μƒν™© | νƒ€μ΄λ¨Έ | μ΄μ  |
|------|--------|------|
| π” AI μ§λ¬Έ μμ„± μ¬μƒ μ¤‘ | βΈ μ •μ§€ | μ‹μ¤ν… μ²λ¦¬ |
| π¤ ν•™μƒμ΄ λ§ν•λ” μ¤‘ | β–¶ μ‘λ™ | **ν•™μƒμ λ‹µλ³€ μ‹κ°„** |
| β³ Whisper μμ„± λ³€ν™ μ¤‘ | βΈ μ •μ§€ | μ‹μ¤ν… μ²λ¦¬ |
| π¤– AI μ§λ¬Έ μƒμ„± μ¤‘ | βΈ μ •μ§€ | μ‹μ¤ν… μ²λ¦¬ |

### κ°„μ†ν™”λ μμ„± λ¨λ“ νλ¦„

```
π” AI μ§λ¬Έ μμ„± μ¬μƒ   β†’  βΈ νƒ€μ΄λ¨Έ μ •μ§€
         β†“
π¤ μλ™ λ§μ΄ν¬ ν™μ„±ν™”   β†’  β–¶ νƒ€μ΄λ¨Έ μ‘λ™ (ν•™μƒμ΄ λ§ν•λ” μ‹κ°„)
   ν•™μƒμ΄ λ§ν•¨
         β†“
β³ μμ„±β†’ν…μ¤νΈ λ³€ν™     β†’  βΈ νƒ€μ΄λ¨Έ μ •μ§€
         β†“
π¤– AI λ‹¤μ μ§λ¬Έ μƒμ„±    β†’  βΈ νƒ€μ΄λ¨Έ μ •μ§€
         β†“
π” AI μ§λ¬Έ μμ„± μ¬μƒ    β†’  βΈ νƒ€μ΄λ¨Έ μ •μ§€ (λ°λ³µ)
```

---

## Files to Create/Modify

| νμΌ | μ„¤λ… | μƒνƒ |
|------|------|------|
| `backend/src/routes/speech.ts` | /tts, /stt, /status | β… |
| `backend/src/services/speech.ts` | ElevenLabs, Whisper ν†µν•© | β… |
| `frontend/hooks/useSpeech.ts` | speak(), startListening(), stopListening() | β… |
| `frontend/components/interview/VoiceInterface.tsx` | μƒνƒ ν‘μ‹ + λ‹µλ³€ μ™„λ£ λ²„νΌ | β… |
| `frontend/components/interview/VolumeVisualizer.tsx` | μ‹¤μ‹κ°„ λ³Όλ¥¨ λ§‰λ€ | β… |
| ~~`frontend/components/interview/RecordButton.tsx`~~ | ~~λ„λ¥΄κ³  λ§ν•κΈ° λ²„νΌ~~ | β μ κ±°λ¨ |

---

## API Endpoints (μ°Έμ΅°: 05-api.md)

### Speech API
| Method | Endpoint | μ„¤λ… |
|--------|----------|------|
| GET | `/api/speech/status` | μ„λΉ„μ¤ μƒνƒ ν™•μΈ |
| POST | `/api/speech/tts` | ν…μ¤νΈ β†’ μμ„± (MP3) |
| POST | `/api/speech/stt` | μμ„± β†’ ν…μ¤νΈ (FormData) |

---

## Key Implementation Details

### TTS μ„λΉ„μ¤ (ElevenLabs)
```typescript
// services/speech.ts
import { ElevenLabsClient } from '@elevenlabs/elevenlabs-js';

const elevenlabs = new ElevenLabsClient({
  apiKey: process.env.ELEVENLABS_API_KEY,
});

export async function textToSpeech(text: string): Promise<Buffer> {
  const voiceId = process.env.ELEVENLABS_VOICE_ID || '4JJwo477JUAx3HV0T7n7';
  const modelId = process.env.ELEVENLABS_MODEL || 'eleven_flash_v2_5';

  const audio = await elevenlabs.textToSpeech.convert(voiceId, {
    text,
    modelId: modelId,           // camelCase ν•„μ
    outputFormat: 'mp3_44100_128',  // camelCase ν•„μ
  });

  // Convert stream to buffer
  const chunks: Buffer[] = [];
  for await (const chunk of audio) {
    chunks.push(Buffer.from(chunk));
  }
  return Buffer.concat(chunks);
}
```

### STT μ„λΉ„μ¤ (Whisper)
```typescript
// services/speech.ts
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export async function speechToText(
  audioBuffer: Buffer,
  context?: string
): Promise<string> {
  // Buffer β†’ Uint8Array β†’ File (νΈν™μ„± λ¬Έμ  ν•΄κ²°)
  const uint8Array = new Uint8Array(audioBuffer);
  const file = new File([uint8Array], 'audio.webm', { type: 'audio/webm' });

  const response = await openai.audio.transcriptions.create({
    file,
    model: 'whisper-1',
    language: 'ko',
    prompt: context, // Context-aware ννΈ
  });

  return response.text;
}
```

### useSpeech ν›… (ν•µμ‹¬ ν›…)
```typescript
// hooks/useSpeech.ts

// TTS ν›…
export function useSpeechSynthesis(sessionToken, options) {
  const [isSpeaking, setIsSpeaking] = useState(false);

  const speak = async (text: string) => {
    const response = await fetch('/api/speech/tts', {
      method: 'POST',
      headers: { Authorization: `Bearer ${sessionToken}` },
      body: JSON.stringify({ text }),
    });
    const blob = await response.blob();
    const audio = new Audio(URL.createObjectURL(blob));
    audio.onended = () => options.onEnd?.();
    await audio.play();
  };

  return { isSpeaking, speak, stop };
}

// STT ν›…
export function useWhisperRecognition(sessionToken, options) {
  const [isListening, setIsListening] = useState(false);
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [volumeLevel, setVolumeLevel] = useState(0);

  const startListening = async (context?: string) => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    // AudioContextλ΅ λ³Όλ¥¨ μ‹κ°ν™”
    // MediaRecorderλ΅ λ…Ήμ
  };

  const stopListening = async (): Promise<string> => {
    // λ…Ήμ μΆ…λ£ β†’ STT API νΈμ¶ β†’ ν…μ¤νΈ λ°ν™
  };

  return { isListening, isTranscribing, volumeLevel, startListening, stopListening };
}

// λ§μ΄ν¬ κ¶ν• μ ν‹Έλ¦¬ν‹°
export async function requestMicrophonePermission(): Promise<boolean> {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    stream.getTracks().forEach(track => track.stop());
    return true;
  } catch {
    return false;
  }
}
```

### λ§μ΄ν¬ κ¶ν• μ‚¬μ „ μ”μ²­ (μ¤€λΉ„ ν™”λ©΄)
```typescript
// app/interview/start/page.tsx

const [micPermission, setMicPermission] = useState<'pending' | 'checking' | 'granted' | 'denied'>('pending');

// μμ„± λ¨λ“ μ„ νƒ μ‹ μ¦‰μ‹ κ¶ν• μ”μ²­
const handleVoiceModeSelect = async () => {
  setMicPermission('checking');
  const granted = await requestMicrophonePermission();

  if (granted) {
    setMicPermission('granted');
    setSelectedMode('voice');
  } else {
    setMicPermission('denied');
    setError('λ§μ΄ν¬ κ¶ν•μ΄ ν•„μ”ν•©λ‹λ‹¤. μ±„ν… λ¨λ“λ¥Ό μ„ νƒν•΄μ£Όμ„Έμ”.');
  }
};

// μΈν„°λ·° μ‹μ‘ λ²„νΌ - κ¶ν• μ—†μΌλ©΄ λΉ„ν™μ„±ν™”
<button
  onClick={handleStart}
  disabled={selectedMode === 'voice' && micPermission !== 'granted'}
>
  μΈν„°λ·° μ‹μ‘
</button>
```

### μΈν„°λ·° νμ΄μ§€ μμ„± λ¨λ“ ν†µν•©
```typescript
// app/interview/page.tsx

// TTS λλ‚λ©΄ μλ™μΌλ΅ λ§μ΄ν¬ μ‹μ‘
const handleTTSEnd = async () => {
  setIsSpeaking(false);
  setTtsFailed(false);
  await startListening(contextRef.current);
};

// μμ„± λ¨λ“ μ΅°κ±΄λ¶€ λ λ”λ§
{isVoiceMode ? (
  <VoiceInterface
    isSpeaking={isSpeaking}
    isListening={isListening}
    isTranscribing={isTranscribing}
    isAiGenerating={isAiGenerating}
    volumeLevel={volumeLevel}
    onCompleteAnswer={handleVoiceSubmit}
    currentQuestion={currentQuestion}
    ttsFailed={ttsFailed}
    reconnected={reconnected}
    onStartListening={handleManualStartListening}
  />
) : (
  <ChatInterface ... />
)}
```

### νƒ€μ΄λ¨Έ μμ • (μμ„± λ¨λ“)
```typescript
// hooks/useInterviewTimer.ts

interface UseInterviewTimerProps {
  // κΈ°μ΅΄ props...
  isSpeaking?: boolean;      // TTS μ¬μƒ μ¤‘
  isTranscribing?: boolean;  // STT λ³€ν™ μ¤‘
}

// νƒ€μ΄λ¨Έ μ‘λ™ μ΅°κ±΄
const shouldTick =
  (isTyping || isTopicStarted) &&
  !isSpeaking &&       // TTS μ¬μƒ μ¤‘ μ •μ§€
  !isTranscribing &&   // STT λ³€ν™ μ¤‘ μ •μ§€
  !aiGenerating;       // AI μƒμ„± μ¤‘ μ •μ§€

// ν•™μƒμ΄ λ§ν•λ” μ¤‘(isListening)μ—λ” νƒ€μ΄λ¨Έ μ‘λ™!
```

---

## UI References (μ°Έμ΅°: 01-pages.md)

### μμ„± λ¨λ“ λ μ΄μ•„μ›ƒ (λ³€κ²½λ¨)
```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚  μ£Όμ  1/3: μ„λ΅  λ° μ—°κµ¬ λ°°κ²½              β± 02:45 β”‚
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¤
β”‚                                                    β”‚
β”‚  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”   β”‚
β”‚  β”‚ π¤– AI                               [π”]   β”‚   β”‚
β”‚  β”‚ "μ—°κµ¬ λ°°κ²½μ—μ„ μ–ΈκΈ‰ν• μ„ ν–‰ μ—°κµ¬μ— λ€ν•΄     β”‚   β”‚
β”‚  β”‚  μ„¤λ…ν•΄ μ£Όμ‹κ² μ–΄μ”?"                       β”‚   β”‚
β”‚  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”   β”‚
β”‚                                                    β”‚
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¤
β”‚                                                    β”‚
β”‚        π”΄ λ…Ήμ μ¤‘   λ³Όλ¥¨ β–β–β–β–β–β–β–β–β–‘β–‘               β”‚
β”‚                                                    β”‚
β”‚              β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”                   β”‚
β”‚              β”‚   λ‹µλ³€ μ™„λ£     β”‚  β† λ§ λλ‚λ©΄ ν΄λ¦­ β”‚
β”‚              β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”                   β”‚
β”‚                                                    β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
```

### μμ„± λ¨λ“ μƒνƒ (λ³€κ²½λ¨)
| μƒνƒ | ν‘μ‹ | λ²„νΌ |
|------|------|------|
| AI μ§λ¬Έ μ¬μƒ μ¤‘ | π” "AIκ°€ λ§ν•κ³  μμµλ‹λ‹¤..." | λΉ„ν™μ„± |
| λ…Ήμ μ¤‘ (μλ™ μ‹μ‘) | π”΄ + λ³Όλ¥¨ λ§‰λ€ | [λ‹µλ³€ μ™„λ£] |
| μμ„± λ³€ν™ μ¤‘ | β³ "μμ„±μ„ λ³€ν™ν•κ³  μμµλ‹λ‹¤..." | λΉ„ν™μ„± |
| AI μ§λ¬Έ μƒμ„± μ¤‘ | π¤– "λ‹¤μ μ§λ¬Έμ„ μ¤€λΉ„ν•κ³  μμµλ‹λ‹¤..." | λΉ„ν™μ„± |

---

## μμ™Έ μΌ€μ΄μ¤ μ²λ¦¬

### 1. μ¬μ ‘μ† μ‹ (μλ™ μ²λ¦¬)
- λ§μ§€λ§‰ AI μ§λ¬Έμ„ ν…μ¤νΈλ΅ ν‘μ‹
- μλ™μΌλ΅ λ§μ΄ν¬ ν™μ„±ν™” (κ¶ν•μ€ μ¤€λΉ„ ν™”λ©΄μ—μ„ μ΄λ―Έ νλ“)
- "μ¬μ ‘μ†λμ—μµλ‹λ‹¤. λ§μ΄ν¬λ¥Ό μ‹μ‘ν•μ„Έμ”." + μλ™ μ‹μ‘ λ²„νΌ

### 2. TTS μ¬μƒ μ‹¤ν¨ μ‹ (μλ™ μ²λ¦¬)
- AI μ§λ¬Έμ„ ν…μ¤νΈλ΅ ν‘μ‹ (fallback)
- μλ™μΌλ΅ λ§μ΄ν¬ μ‹μ‘ (λ²„νΌ μ—†μ)
- "μμ„± μ¬μƒ μ‹¤ν¨" κ²½κ³  ν‘μ‹

### 3. λ§μ΄ν¬ κ¶ν• κ±°λ¶€ μ‹
- μ¤€λΉ„ ν™”λ©΄μ—μ„ μ²λ¦¬
- μ—λ¬ λ©”μ‹μ§€ ν‘μ‹
- μ±„ν… λ¨λ“λ§ μ„ νƒ κ°€λ¥

---

## ν™κ²½ λ³€μ μ„¤μ • (backend/.env)

```bash
# ElevenLabs TTS
ELEVENLABS_API_KEY=sk_xxxxxxxxxxxxxxxxxxxxxxxx
ELEVENLABS_VOICE_ID=4JJwo477JUAx3HV0T7n7
ELEVENLABS_MODEL=eleven_flash_v2_5
```

---

## Notes
- ElevenLabs API νλΌλ―Έν„°: `modelId`, `outputFormat` (camelCase ν•„μ)
- Buffer β†’ File λ³€ν™ μ‹ Uint8Array μ‚¬μ© (νΈν™μ„±)
- λΈλΌμ°μ € νΈν™μ„±: MediaRecorder μ§€μ› ν™•μΈ
- λ§μ΄ν¬ κ¶ν•: μ¤€λΉ„ ν™”λ©΄μ—μ„ μ‚¬μ „ μ”μ²­ ν•„μ
